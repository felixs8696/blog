---
layout: post
title: Asynchronous Methods for Deep Reinforcement Learning
categories: reinforcement_learning
author: "Felix Su"
published: false
---

## Table of Contents
{:.no_toc}
* TOC
{:toc}

## Paper

- [[Minh et al. 2016] Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783.pdf){:target="_blank"}


## Issue References
- [Time Correlation and Non-Stationarity: Common Issues with RL and The Methods That Attempt to Solve Them: Issue #1]({{site.baseurl}}/reinforcement_learning/2018/12/20/common-issues-with-reinforcement-learning.html#issue-1)

## Objective
Experience replay uses **more memory and computation per interaction** and **requires an off-policy algorithm** to update from data generated by an older policy. These authors propose an alternative method to **asynchronously execute multiple agents in parallel** on multiple instances of the environment. This has a similar effect as experience replay because at any given time step, the parallel agents experience a wide variety of different states, thus decorrelating the agent's data into a more stationary processes.
